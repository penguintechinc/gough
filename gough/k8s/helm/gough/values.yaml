# Gough Hypervisor Automation System - Helm Chart Values
# Enterprise production configuration for 100+ server deployments
# High availability, scalability, and operational excellence

# =================================
# GLOBAL CONFIGURATION
# =================================

global:
  # Container image settings
  imageRegistry: "docker.io"
  imagePullPolicy: IfNotPresent
  storageClass: "fast-ssd"
  
  # Network settings
  domain: "gough.local"
  tlsSecretName: "gough-tls"
  
  # Security
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
  
  # Resource limits
  resources:
    small:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "256Mi"
        cpu: "200m"
    medium:
      requests:
        memory: "256Mi"
        cpu: "200m"
      limits:
        memory: "512Mi"
        cpu: "500m"
    large:
      requests:
        memory: "512Mi"
        cpu: "500m"
      limits:
        memory: "1Gi"
        cpu: "1000m"
    xlarge:
      requests:
        memory: "1Gi"
        cpu: "1000m"
      limits:
        memory: "2Gi"
        cpu: "2000m"

# =================================
# MAAS CONFIGURATION
# =================================

maas:
  enabled: true
  replicaCount: 2  # HA deployment
  
  image:
    repository: "gough/maas"
    tag: "3.4-ha"
    pullPolicy: IfNotPresent
  
  # HA configuration
  ha:
    enabled: true
    mode: "region"  # region, rack, or both
    clustering: true
    
  # Region controller settings
  region:
    replicas: 2
    resources: xlarge
    
    # Database connection
    database:
      host: "postgres-primary"
      port: 5432
      name: "maasdb"
      user: "maas"
      passwordSecret: "maas-db-secret"
      
    # Shared storage for images
    storage:
      class: "nfs-client"
      size: "500Gi"
      accessModes:
        - ReadWriteMany
  
  # Rack controller settings
  rack:
    replicas: 3
    resources: large
    
    # Network configuration
    network:
      dhcp:
        enabled: true
        subnet: "192.168.100.0/24"
        rangeStart: "192.168.100.10"
        rangeEnd: "192.168.100.250"
        gateway: "192.168.100.1"
        dns: "8.8.8.8,8.8.4.4"
      pxe:
        interface: "eth0"
  
  # Service configuration
  service:
    type: LoadBalancer
    ports:
      web: 5240
      api: 5241
      rack: 5242
      metadata: 5247
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
  
  # Ingress configuration
  ingress:
    enabled: true
    className: "nginx"
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: "10m"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    hosts:
      - host: "maas.gough.local"
        paths:
          - path: "/"
            pathType: Prefix
    tls:
      - secretName: "maas-tls"
        hosts:
          - "maas.gough.local"

# =================================
# MANAGEMENT SERVER CONFIGURATION
# =================================

managementServer:
  enabled: true
  replicaCount: 3  # HA deployment
  
  image:
    repository: "gough/management-server"
    tag: "1.0.0"
    pullPolicy: IfNotPresent
  
  resources: large
  
  # Environment configuration
  config:
    debug: false
    secretKey: "gough-secret-key"  # Will be generated
    database:
      url: "postgresql://postgres:password@postgres:5432/management"
    redis:
      url: "redis://redis:6379/0"
    maas:
      url: "http://maas-region:5240/MAAS/"
      apiKey: "maas-api-key"  # From secret
    fleet:
      url: "https://fleetdm:8443"
      apiKey: "fleet-api-key"  # From secret
  
  # Service configuration
  service:
    type: ClusterIP
    port: 8000
  
  # Ingress configuration
  ingress:
    enabled: true
    className: "nginx"
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: "50m"
      nginx.ingress.kubernetes.io/websocket-services: "management-server"
    hosts:
      - host: "gough.local"
        paths:
          - path: "/"
            pathType: Prefix
    tls:
      - secretName: "gough-tls"
        hosts:
          - "gough.local"
  
  # Horizontal Pod Autoscaler
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80

# =================================
# FLEETDM CONFIGURATION
# =================================

fleetdm:
  enabled: true
  replicaCount: 2  # HA deployment
  
  image:
    repository: "fleetdm/fleet"
    tag: "v4.40.0"
    pullPolicy: IfNotPresent
  
  resources: large
  
  # Configuration
  config:
    mysql:
      host: "mysql"
      port: 3306
      database: "fleet"
      user: "fleet"
      passwordSecret: "mysql-secret"
    redis:
      host: "redis"
      port: 6379
    tls:
      enabled: true
      certSecret: "fleet-tls"
  
  # Service configuration
  service:
    type: ClusterIP
    ports:
      http: 8080
      https: 8443
  
  # Ingress configuration
  ingress:
    enabled: true
    className: "nginx"
    annotations:
      nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
      nginx.ingress.kubernetes.io/proxy-body-size: "100m"
    hosts:
      - host: "fleet.gough.local"
        paths:
          - path: "/"
            pathType: Prefix
    tls:
      - secretName: "fleet-tls"
        hosts:
          - "fleet.gough.local"

# =================================
# DATABASE CONFIGURATION
# =================================

postgresql:
  enabled: true
  auth:
    postgresPassword: "postgres"
    username: "maas"
    password: "maaspassword"
    database: "maasdb"
  
  architecture: replication
  replication:
    enabled: true
    user: "replica"
    password: "replicapassword"
    numSynchronousReplicas: 1
  
  primary:
    resources: xlarge
    persistence:
      enabled: true
      size: "100Gi"
      storageClass: "fast-ssd"
    configuration: |
      # High availability configuration
      wal_level = replica
      max_wal_senders = 3
      max_replication_slots = 3
      hot_standby = on
      shared_buffers = 256MB
      effective_cache_size = 1GB
      maintenance_work_mem = 64MB
      checkpoint_completion_target = 0.9
      wal_buffers = 16MB
      default_statistics_target = 100
      random_page_cost = 1.1
      effective_io_concurrency = 200
      work_mem = 4MB
      min_wal_size = 1GB
      max_wal_size = 4GB
  
  readReplicas:
    replicaCount: 2
    resources: large
    persistence:
      enabled: true
      size: "100Gi"
      storageClass: "fast-ssd"

redis:
  enabled: true
  architecture: replication
  auth:
    enabled: true
    password: "redispassword"
  
  master:
    resources: medium
    persistence:
      enabled: true
      size: "10Gi"
      storageClass: "fast-ssd"
  
  replica:
    replicaCount: 2
    resources: medium
    persistence:
      enabled: true
      size: "10Gi"
      storageClass: "fast-ssd"

mysql:
  enabled: true
  auth:
    rootPassword: "rootpassword"
    username: "fleet"
    password: "fleetpassword"
    database: "fleet"
  
  architecture: replication
  secondary:
    replicaCount: 1
  
  primary:
    resources: large
    persistence:
      enabled: true
      size: "50Gi"
      storageClass: "fast-ssd"

# =================================
# MONITORING STACK
# =================================

monitoring:
  enabled: true
  
  prometheus:
    enabled: true
    server:
      resources: xlarge
      retention: "30d"
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: "fast-ssd"
            resources:
              requests:
                storage: "100Gi"
    
    alertmanager:
      enabled: true
      resources: medium
      config:
        global:
          slack_api_url: ""  # Configure in secrets
        route:
          group_by: ['alertname']
          group_wait: 10s
          group_interval: 10s
          repeat_interval: 1h
          receiver: 'web.hook'
        receivers:
          - name: 'web.hook'
            slack_configs:
              - channel: '#alerts'
                title: 'Gough Alert'
                text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
    
    nodeExporter:
      enabled: true
    
    kubeStateMetrics:
      enabled: true
  
  grafana:
    enabled: true
    admin:
      user: admin
      password: "grafana123"  # Change in production
    
    resources: medium
    
    persistence:
      enabled: true
      size: "10Gi"
      storageClass: "fast-ssd"
    
    ingress:
      enabled: true
      hosts:
        - "grafana.gough.local"
      tls:
        - secretName: "grafana-tls"
          hosts:
            - "grafana.gough.local"
    
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Prometheus
            type: prometheus
            url: http://prometheus:9090
            access: proxy
            isDefault: true
          - name: Elasticsearch
            type: elasticsearch
            url: http://elasticsearch:9200
            access: proxy
            database: "logstash-*"

# =================================
# LOGGING STACK
# =================================

logging:
  enabled: true
  
  elasticsearch:
    enabled: true
    clusterName: "gough-logging"
    nodeGroup: "master"
    masterService: "elasticsearch-master"
    
    replicas: 3
    minimumMasterNodes: 2
    
    resources:
      requests:
        memory: "2Gi"
        cpu: "1000m"
      limits:
        memory: "4Gi"
        cpu: "2000m"
    
    volumeClaimTemplate:
      accessModes: ["ReadWriteOnce"]
      storageClassName: "fast-ssd"
      resources:
        requests:
          storage: "100Gi"
    
    esConfig:
      elasticsearch.yml: |
        cluster.name: "gough-logging"
        network.host: 0.0.0.0
        discovery.zen.minimum_master_nodes: 2
        discovery.zen.ping.unicast.hosts: "elasticsearch-master-headless"
  
  kibana:
    enabled: true
    
    resources: large
    
    ingress:
      enabled: true
      hosts:
        - host: "kibana.gough.local"
          paths:
            - path: "/"
      tls:
        - secretName: "kibana-tls"
          hosts:
            - "kibana.gough.local"
    
    kibanaConfig:
      kibana.yml: |
        elasticsearch.hosts: ["http://elasticsearch:9200"]
        server.host: 0.0.0.0
        server.basePath: ""
        xpack.security.enabled: false
        xpack.monitoring.enabled: true
  
  logstash:
    enabled: true
    replicas: 2
    
    resources: xlarge
    
    logstashConfig:
      logstash.yml: |
        http.host: 0.0.0.0
        xpack.monitoring.elasticsearch.hosts: ["http://elasticsearch:9200"]
    
    logstashPipeline:
      logstash.conf: |
        input {
          beats {
            port => 5044
          }
          tcp {
            port => 5000
            codec => json
          }
          udp {
            port => 5001
            codec => json
          }
        }
        
        filter {
          if [kubernetes] {
            mutate {
              rename => { "[kubernetes][pod][name]" => "pod_name" }
              rename => { "[kubernetes][namespace]" => "namespace" }
              rename => { "[kubernetes][container][name]" => "container_name" }
            }
          }
          
          # Parse JSON logs
          if [message] =~ /^\{/ {
            json {
              source => "message"
            }
          }
          
          # Add timestamp
          date {
            match => [ "timestamp", "ISO8601" ]
          }
          
          # Geoip for IP addresses
          if [client_ip] {
            geoip {
              source => "client_ip"
            }
          }
        }
        
        output {
          elasticsearch {
            hosts => ["elasticsearch:9200"]
            index => "gough-logs-%{+YYYY.MM.dd}"
          }
        }
  
  filebeat:
    enabled: true
    
    daemonset:
      resources: medium
    
    filebeatConfig:
      filebeat.yml: |
        filebeat.inputs:
        - type: container
          paths:
            - /var/log/containers/*.log
          processors:
          - add_kubernetes_metadata:
              host: ${NODE_NAME}
              matchers:
              - logs_path:
                  logs_path: "/var/log/containers/"
        
        output.logstash:
          hosts: ["logstash:5044"]
        
        processors:
        - drop_fields:
            fields: ["beat", "input_type", "source", "offset"]

# =================================
# NETWORKING CONFIGURATION
# =================================

networking:
  # Service mesh (optional)
  istio:
    enabled: false
  
  # Network policies
  networkPolicies:
    enabled: true
    
    # Default deny all
    defaultDeny: true
    
    # Allow specific communications
    rules:
      - name: "allow-dns"
        podSelector: {}
        egress:
          - to: []
            ports:
              - protocol: UDP
                port: 53
      
      - name: "allow-management-to-db"
        podSelector:
          matchLabels:
            app: management-server
        egress:
          - to:
              - podSelector:
                  matchLabels:
                    app: postgresql
            ports:
              - protocol: TCP
                port: 5432

# =================================
# STORAGE CONFIGURATION
# =================================

storage:
  # Storage classes
  classes:
    fast-ssd:
      provisioner: "kubernetes.io/aws-ebs"
      parameters:
        type: "gp3"
        fsType: "ext4"
        encrypted: "true"
      reclaimPolicy: Retain
      volumeBindingMode: WaitForFirstConsumer
    
    standard:
      provisioner: "kubernetes.io/aws-ebs"
      parameters:
        type: "gp2"
        fsType: "ext4"
      reclaimPolicy: Delete
      volumeBindingMode: Immediate
  
  # Persistent volumes
  volumes:
    maas-images:
      size: "500Gi"
      storageClass: "nfs-client"
      accessModes:
        - ReadWriteMany

# =================================
# SECURITY CONFIGURATION
# =================================

security:
  # Pod Security Standards
  podSecurityStandard: "restricted"
  
  # Service accounts
  serviceAccount:
    create: true
    annotations: {}
    name: "gough"
  
  # RBAC
  rbac:
    create: true
    rules:
      - apiGroups: [""]
        resources: ["pods", "services", "endpoints"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["apps"]
        resources: ["deployments", "replicasets"]
        verbs: ["get", "list", "watch"]
  
  # Network security
  networkPolicies:
    enabled: true
  
  # Secrets management
  secrets:
    # Database passwords
    postgresql:
      enabled: true
      data:
        postgres-password: ""  # Will be generated
        password: ""           # Will be generated
        replication-password: ""  # Will be generated
    
    # Redis password
    redis:
      enabled: true
      data:
        redis-password: ""  # Will be generated
    
    # MySQL passwords
    mysql:
      enabled: true
      data:
        mysql-root-password: ""  # Will be generated
        mysql-password: ""       # Will be generated
    
    # API keys
    api-keys:
      enabled: true
      data:
        maas-api-key: ""   # Configure manually
        fleet-api-key: ""  # Configure manually

# =================================
# BACKUP AND DISASTER RECOVERY
# =================================

backup:
  enabled: true
  
  # Velero backup
  velero:
    enabled: true
    schedule: "0 2 * * *"  # Daily at 2 AM
    ttl: "720h"            # 30 days retention
    
    includedNamespaces:
      - "gough"
    
    excludedResources:
      - "pods"
      - "events"

# =================================
# INGRESS CONFIGURATION
# =================================

ingress:
  # Ingress controller
  nginx:
    enabled: true
    controller:
      replicaCount: 3
      resources: large
      
      config:
        proxy-body-size: "100m"
        proxy-read-timeout: "300"
        proxy-send-timeout: "300"
        client-body-buffer-size: "1m"
        client-body-timeout: "60"
        client-header-timeout: "60"
        keepalive-timeout: "75"
        large-client-header-buffers: "4 8k"
        
      service:
        type: LoadBalancer
        annotations:
          service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
          service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
  
  # TLS configuration
  tls:
    enabled: true
    certManager:
      enabled: true
      issuer: "letsencrypt-prod"

# =================================
# SCALING AND PERFORMANCE
# =================================

scaling:
  # Horizontal Pod Autoscaler
  hpa:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 70
      - type: Resource
        resource:
          name: memory
          target:
            type: Utilization
            averageUtilization: 80
  
  # Vertical Pod Autoscaler
  vpa:
    enabled: false  # Enable if available in cluster
  
  # Pod Disruption Budgets
  pdb:
    enabled: true
    minAvailable: 50%

# =================================
# NODE AFFINITY AND SCHEDULING
# =================================

scheduling:
  # Node affinity
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: "kubernetes.io/arch"
              operator: In
              values: ["amd64"]
  
  # Pod anti-affinity for HA
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: "app"
                operator: In
                values: ["gough"]
          topologyKey: "kubernetes.io/hostname"
  
  # Tolerations
  tolerations: []

# =================================
# MAINTENANCE AND UPDATES
# =================================

maintenance:
  # Update strategy
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  
  # Maintenance windows
  windows:
    enabled: true
    schedule: "0 2 * * SUN"  # Sunday 2 AM
    duration: "4h"